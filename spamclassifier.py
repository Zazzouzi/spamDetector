# -*- coding: utf-8 -*-
"""SpamClassifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1doavHS6J9v2xgRmzb7vo5x05j8b1Midy
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
spambase = fetch_ucirepo(id=94)

# data (as pandas dataframes)
X = spambase.data.features
print("X")
print(X)
y = spambase.data.targets
print("y")
print(y)

#putting the data into dataframes, one for the features and another for the target values
import pandas as pd

df = pd.DataFrame(spambase.data.features)
# print(df)
df_2 = pd.DataFrame(spambase.data.targets)
print(df_2.values)

#below we are using a correlation matrix to find the correlation between all the features
#For example by looking at the matrix we can see that the correlation coefficient between features word_freq_make
#and word_freq_our is .023119

# the correlation that we care about is the correlation between the spam truth value and the other features. We
#want to answer the question, what features give us the most information about whether or not the email is spam
df.insert(0, "Spam Truth Value", df_2.values)

df.corr("pearson")

import numpy as np
from matplotlib import pyplot as plt


df_feature = pd.DataFrame()
df_feature.insert(0, "char_freq_$", df["char_freq_$"].values)
df_feature.insert(0, "Spam Truth Value", df["Spam Truth Value"].values)
df_feature.insert(0, "word_freq_your", df["word_freq_your"].values)

# plotting feature 1 (occurance of $)
x_1 = range(0, 1813)
x_2 = range(0, 2788)
y_1 = df_feature[df_feature['Spam Truth Value'] == 1]
y_2 = df_feature[df_feature['Spam Truth Value'] == 0]

plt.plot(x_1, y_1["char_freq_$"].values, color="k", label="Spam Email")  # black = where it IS a spam email
plt.plot(x_2, y_2["char_freq_$"].values, color="y", label="Not Spam Email")  # yellow = where it is NOT a spam email
plt.xlabel("Email index")
plt.ylabel("# of times $ occurs")
plt.legend()
plt.show()
#the code above makes the plot in black and yellow. The plot shows that spam emails have siginicantly higher counts
#of '$' occurences. This makes sense as spam emails often promise money or warn that you have lost money. This
#is important because it will serve as a helpful indicator of what instances could be spam

# Plotting feature 2 (occurance of "your")
y_3 = df_feature[df_feature['Spam Truth Value'] == 1]
y_4 = df_feature[df_feature['Spam Truth Value'] == 0]

plt.plot(x_1, y_3["word_freq_your"].values, color="r", label="Spam Email")  # red = where it IS a spam email
plt.plot(x_2, y_4["word_freq_your"].values, color="g", label="Not Spam Email")  # green = where it is NOT a spam email
plt.xlabel("Email index")
plt.ylabel("# of times \"your\" occurs")
plt.legend()
plt.show()

#the code above makes the plot in red and green. The plot shows many spam emails have high counts
#of 'your' occurences. This makes sense as 'your' is a word that often shows up in text. This plot does not show
#as clear a difference between spam and normal emails but it does show that many of the spam emails have higher
#rates of 'your' occurences despite here being instances of normal emails with even higher 'your' counts.

# Make a dataset (df_3) consisting of only the features we need
#after running a cell with a model (logistic regression, SVC, Rando forest, etc) you need to run this cell again to re-populate df_3

features_list = [
    "word_freq_make",
    "word_freq_address",
    "word_freq_all",
    "word_freq_3d",
    "word_freq_our",
    "word_freq_over",
    "word_freq_remove",
    "word_freq_internet",
    "word_freq_order",
    "word_freq_mail",
    "word_freq_receive",
    "word_freq_will",
    "word_freq_people",
    "word_freq_report",
    "word_freq_addresses",
    "word_freq_free",
    "word_freq_business",
    "word_freq_email",
    "word_freq_you",
    "word_freq_credit",
    "word_freq_your",
    "word_freq_font",
    "word_freq_000",
    "word_freq_money",
    "word_freq_hp",
    "word_freq_hpl",
    "word_freq_george",
    "word_freq_650",
    "word_freq_lab",
    "word_freq_labs",
    "word_freq_telnet",
    "word_freq_857",
    "word_freq_data",
    "word_freq_415",
    "word_freq_85",
    "word_freq_technology",
    "word_freq_1999",
    "word_freq_parts",
    "word_freq_pm",
    "word_freq_direct",
    "word_freq_cs",
    "word_freq_meeting",
    "word_freq_original",
    "word_freq_project",
    "word_freq_re",
    "word_freq_edu",
    "word_freq_table",
    "word_freq_conference",
    "char_freq_;",
    "char_freq_(",
    "char_freq_[",
    "char_freq_!",
    "char_freq_$",
    "char_freq_#",
    "capital_run_length_average",
    "capital_run_length_longest",
    "capital_run_length_total",
    "Spam Truth Value"
]

df_3 = pd.DataFrame()


for feature in features_list:
    df_3.insert(0, feature, df[feature].values)

#using logistic regression
from sklearn.model_selection import train_test_split
import random
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import recall_score


y = df_3.pop("Spam Truth Value")  # y is the grounded truth value (ie, whether or not it was spam)
x = df_3  # x is all the other features we've deemed "worthy"

X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)

model = LogisticRegression()

#fitting model and using it for predictions
model.fit(X_train, y_train)
y_predicted = model.predict(X_test)

matrix = confusion_matrix(y_test, y_predicted)
print(f"Here we have our Confusion Matrix for the basic model:\n\n{matrix}")

#FUNCTION THAT PRINTS PROBABILITIES IN X_TEST BEING IN ONE CLASS OR THE OTHER
accuracy = model.score(X_test, y_test)
print("accuracy")
print(accuracy)

recall_rate = recall_score(y_test, y_predicted)
print("Recall is: ", recall_rate )

#using KNN
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix
from sklearn.metrics import precision_score
from sklearn.model_selection import GridSearchCV

y = df_3.pop("Spam Truth Value")
x = df_3

X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)

#find a good number of neighbors to optimize model
param_grid = {'n_neighbors': list(range(1, 50, 2))}  # Test odd values of k
grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring='precision')
grid.fit(X_train, y_train)

optimal_k = grid.best_params_['n_neighbors']
print("the best k found was ", optimal_k)
model = KNeighborsClassifier(n_neighbors=optimal_k)
model.fit(X_train, y_train)
y_predicted = model.predict(X_test)

matrix = confusion_matrix(y_test, y_predicted)
print(f"Here we have our Confusion Matrix for the basic model:\n\n{matrix}")

accuracy = model.score(X_test, y_test)
print("Accuracy is: ", accuracy )
recall_rate = recall_score(y_test, y_predicted)
print("Recall is: ", recall_rate )
precision = precision_score(y_test, y_predicted)
print("Precision is: ", precision)

#using Support Vector Machines
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix

y = df_3.pop("Spam Truth Value")
x = df_3

X_train, X_test, y_train, y_test = train_test_split(x, y, random_state=42)

model = SVC(C=0.1, class_weight='balanced')

#fitting model and using it for predictions
model.fit(X_train, y_train)
y_predicted = model.predict(X_test)

matrix = confusion_matrix(y_test, y_predicted)
print(f"Here we have our Confusion Matrix for the basic model:\n\n{matrix}")

accuracy = model.score(X_test, y_test)
print("Accuracy is: ", accuracy )
recall_rate = recall_score(y_test, y_predicted)
print("Recall is: ", recall_rate )

#using Random Forest Classifier
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, recall_score, confusion_matrix

y = df_3.pop('Spam Truth Value')
X = df_3

# spliitting the dataset, and making the test size
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Training on the full feature list
full_model = RandomForestClassifier(n_estimators=100, random_state=42)
full_model.fit(X_train, y_train)
y_pred_full = full_model.predict(X_test)
accuracy_full = accuracy_score(y_test, y_pred_full)
recall_full = recall_score(y_test, y_pred_full)
confusion_full = confusion_matrix(y_test, y_pred_full)

# figuring out the top 10 features to see how to reduce having to use so many features
feature_importances = full_model.feature_importances_
features_df = pd.DataFrame({
    'Feature': X_train.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)
top_features = features_df.head(10)['Feature']

# Training on the reduced amout of features.
X_train_reduced = X_train[top_features]
X_test_reduced = X_test[top_features]
reduced_model = RandomForestClassifier(n_estimators=100, random_state=42)
reduced_model.fit(X_train_reduced, y_train)
y_predicted_reduced = reduced_model.predict(X_test_reduced)
accuracy_reduced = accuracy_score(y_test, y_predicted_reduced)
recall_reduced = recall_score(y_test, y_predicted_reduced)
confusion_reduced = confusion_matrix(y_test, y_predicted_reduced)

# Comparing the results
print("Full Feature Model:")
print(f"Accuracy: {accuracy_full}")
print(f"Recall: {recall_full}")
print(f"Confusion Matrix:\n{confusion_full}\n")


print(features_df)

print("Top 10 Features Model:")
print(f"Accuracy: {accuracy_reduced}")
print(f"Recall: {recall_reduced}")
print(f"Confusion Matrix:\n{confusion_reduced}")

import pandas as pd
# an example showing how it pick out spam
# seeing how there is not much taken away when we use the top ten over the full feature list, this is what I will be using instead of the full feature list
email_data = {
    'word_freq_money': [0],
    'word_freq_remove': [0],
    'word_freq_free': [1],
    'word_freq_hp': [0],
    'word_freq_your': [3],
    'char_freq_$': [0],
    'char_freq_!': [4],
    'capital_run_length_average': [4],
    'capital_run_length_longest': [1],
    'capital_run_length_total': [20]
}

email_df = pd.DataFrame(email_data)


is_spam = reduced_model.predict(email_df)[0]  # 0 means not spam, 1 means spam


print("The email is predicted as:", "Spam" if is_spam else "Not Spam")